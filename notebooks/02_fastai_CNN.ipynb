{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8hF/j0twyMcFBCfCyNAQu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-Bioinfo-CEITEC/ECCB2022/blob/main/notebooks/02_fastai_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install genomic-benchmarks torchmetrics --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhxLEguoXNNa",
        "outputId": "fc63715d-8fa3-4bfa-eb66-5efa33f0af46"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 419 kB 44.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 30.7 MB/s \n",
            "\u001b[?25h  Building wheel for genomic-benchmarks (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "nucleotide_to_number = {\n",
        "    'A':0,\n",
        "    'C':1,\n",
        "    'T':2,\n",
        "    'G':3,\n",
        "    'N':4,\n",
        "}\n",
        "\n",
        "def numericalize(x, vocab=nucleotide_to_number):\n",
        "  x = [vocab[s] for s in x]\n",
        "  return x"
      ],
      "metadata": {
        "id": "8YrdPyqf9BL0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from genomic_benchmarks.dataset_getters.pytorch_datasets import HumanNontataPromoters\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dset = HumanNontataPromoters('train')\n",
        "test_dset = HumanNontataPromoters('test')\n",
        "\n",
        "def preprocess(batch):\n",
        "  xs, ys = [], []\n",
        "  for x,y in batch:\n",
        "    xs.append(numericalize(x))\n",
        "    ys.append([y])\n",
        "  \n",
        "  return torch.tensor(xs), torch.tensor(ys).float()\n",
        "  \n",
        "train_loader = DataLoader(train_dset, batch_size=32, shuffle=True, collate_fn=preprocess)  \n",
        "test_loader = DataLoader(test_dset, batch_size=32, collate_fn=preprocess)  \n"
      ],
      "metadata": {
        "id": "LGutIXeTXMlw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "VT4Z9-p-zgiQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class FullyConv(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "      super().__init__()\n",
        "      self.net = nn.Sequential(\n",
        "          nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=5, stride=1),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv1d(in_channels=hidden_dim, out_channels=hidden_dim, kernel_size=5, stride=1),\n",
        "          nn.ReLU(),\n",
        "          nn.Flatten(),\n",
        "          nn.LazyLinear(out_features=output_dim), #Lazy layer allows us to skip the in_features parameter and derive it automatically\n",
        "          nn.Sigmoid(),\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = F.one_hot(x, num_classes=5).float()\n",
        "      x = x.transpose(1, 2) #Transposig because Convolutional layers expect channels to be the second dimension. [32, 251, 5] -> [32, 5, 251]\n",
        "      x = self.net(x)\n",
        "      return x\n",
        "\n",
        "net = FullyConv(5,30,1)#.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import Accuracy\n",
        "\n",
        "acc = Accuracy().to('cuda')\n",
        "def test_accuracy(x,y):\n",
        "  return acc(x, y.int())"
      ],
      "metadata": {
        "id": "mSQP64kdT6JT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.text.all import *\n",
        "\n",
        "data = DataLoaders(train_loader, test_loader)\n",
        "# learn = Learner(data, net, loss_func=F.binary_cross_entropy, opt_func=SGD, metrics=[accuracy, BalancedAccuracy(), test_accuracy])\n",
        "learn = Learner(data, net, loss_func=F.binary_cross_entropy, opt_func=SGD, metrics=[test_accuracy])\n"
      ],
      "metadata": {
        "id": "7o6UtCoM9ehW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learn.fit_one_cycle(10, 1e-2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "0HSjh6Tq9yUB",
        "outputId": "96257c00-466c-4d7b-f025-47b350472f8f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>test_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.627349</td>\n",
              "      <td>0.610222</td>\n",
              "      <td>0.716958</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.467112</td>\n",
              "      <td>0.504875</td>\n",
              "      <td>0.762232</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.428674</td>\n",
              "      <td>0.445241</td>\n",
              "      <td>0.789905</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.400784</td>\n",
              "      <td>0.405004</td>\n",
              "      <td>0.818685</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.372335</td>\n",
              "      <td>0.379104</td>\n",
              "      <td>0.833961</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.361406</td>\n",
              "      <td>0.373024</td>\n",
              "      <td>0.841488</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.357887</td>\n",
              "      <td>0.363064</td>\n",
              "      <td>0.845915</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.338035</td>\n",
              "      <td>0.360716</td>\n",
              "      <td>0.844255</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.353169</td>\n",
              "      <td>0.359559</td>\n",
              "      <td>0.845362</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.342356</td>\n",
              "      <td>0.359502</td>\n",
              "      <td>0.846580</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# learn.validate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "WmvShBpbxfiC",
        "outputId": "4005b3b6-77f8-416e-e057-681c7d1d8221"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#2) [0.35950160026550293,0.8465796113014221]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ]
}