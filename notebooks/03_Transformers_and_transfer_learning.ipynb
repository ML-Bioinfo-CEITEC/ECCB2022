{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-Bioinfo-CEITEC/ECCB2022/blob/main/notebooks/03_Transformers_and_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7DUHcKNLkJp"
      },
      "source": [
        "# Basic transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF_kvRbryibQ"
      },
      "source": [
        "## Setup & Data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIcaZLWXy12J"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4vZiFzjt7ZtV"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets tokenizers --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lrwpPk9y_AX"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB3x6DnHaDuP",
        "outputId": "550438b7-73b2-455d-d248-2d481224255d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration simecek--human_nontata_promoters-5bc25970767ab8f4\n",
            "WARNING:datasets.builder:Reusing dataset parquet (/root/.cache/huggingface/datasets/simecek___parquet/simecek--human_nontata_promoters-5bc25970767ab8f4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['labels', 'seq'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "DATASET_NAME = \"simecek/human_nontata_promoters\"\n",
        "\n",
        "# take a small portion of the dataset for time purposes\n",
        "# the fist and the last 500 samples because this specific dataset is ordered (positive, negative samples)\n",
        "dataset_train = load_dataset(DATASET_NAME, split='train[:500]+train[-500:]')\n",
        "dataset_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9aq8AMIU7E-",
        "outputId": "20cf9659-c06c-4b9b-e066-75c1a031f683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration simecek--human_nontata_promoters-5bc25970767ab8f4\n",
            "WARNING:datasets.builder:Reusing dataset parquet (/root/.cache/huggingface/datasets/simecek___parquet/simecek--human_nontata_promoters-5bc25970767ab8f4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['labels', 'seq'],\n",
              "    num_rows: 2000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "dataset_test = load_dataset(DATASET_NAME, split='test[:1000]+test[-1000:]')\n",
        "dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fa9tyYQqkrH",
        "outputId": "ae23586a-cae6-4719-eae8-07285670bf97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 0,\n",
              " 'seq': 'ACAGATTCAGGATGTCCTGTCGGGGCATGGACCCTGGAAAGCTGCGGACACCAGGAGGGCAGGCAAGAGAGTCTCATCTCTTGCTCCCTAGGAGCTATGAGTTGAGGGCGCCGTCTGAGCAGGAGGGACGGACGGGTGCCCAGGGTTTGAGGAAAGAGGGGTGTGGGAAGGACGCATGCTAGAACTTCAGAGCAGTTCAGCAGGTGCAGAATGGGAGTTATCATGGGGACTGTGGGAGAAGGGGCGGTGGG'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# one training sample sequence and its label\n",
        "dataset_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXMgg43Fypoy"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLnNCZbi69ba"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzhurWwCgO2s"
      },
      "source": [
        "You can find useful models and their respective tokenizers right on the Hugging Face repository.\n",
        "\n",
        "You can use the premade tokenizers even for your own model if they fit your task/purpose.\n",
        "\n",
        "If you don't find existing tokenizers for your usecase, the Hugging Face documentation contains simple tutorials to create your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i9ZGgWM_YF28"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"armheb/DNA_bert_6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt-5htSY-5OT"
      },
      "source": [
        "The tokenizers expects sequences of tokens separated by spaces. \n",
        "\n",
        "Therefore, we will move a sliding window along the sequence and extract k-mers of k=6 (6 characters long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_9kMLG0Smcck"
      },
      "outputs": [],
      "source": [
        "def kmers(s, k=6):\n",
        "  return [s[i:i + k] for i in range(0, len(s)-k+1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZNz-o7C_Tcc"
      },
      "source": [
        "Our k-mers function turns a sequence into a list of k-mers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJQrL17nspOJ",
        "outputId": "fea64833-20fe-42f4-f161-b1edc941527d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ATGGAAAGAGGCACCATTCT\n",
            "['ATGGAA', 'TGGAAA', 'GGAAAG', 'GAAAGA', 'AAAGAG', 'AAGAGG', 'AGAGGC', 'GAGGCA', 'AGGCAC', 'GGCACC', 'GCACCA', 'CACCAT', 'ACCATT', 'CCATTC', 'CATTCT']\n"
          ]
        }
      ],
      "source": [
        "example = 'ATGGAAAGAGGCACCATTCT'\n",
        "print(example)\n",
        "\n",
        "example = kmers(example)\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE4Jy-I5jn8b"
      },
      "source": [
        "We are not done yet. \n",
        "\n",
        "Now we have to concatinate the k-mers separated by spaces into a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yldFD_v7j4nd",
        "outputId": "355125ae-53b4-431f-8bef-bf8d98860b3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ATGGAAAGAGGCACCATTCT\n",
            "ATGGAA TGGAAA GGAAAG GAAAGA AAAGAG AAGAGG AGAGGC GAGGCA AGGCAC GGCACC GCACCA CACCAT ACCATT CCATTC CATTCT\n"
          ]
        }
      ],
      "source": [
        "example = 'ATGGAAAGAGGCACCATTCT'\n",
        "print(example)\n",
        "\n",
        "example_kmers = \" \".join(kmers(example))\n",
        "print(example_kmers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9BSy2M7kv2U"
      },
      "source": [
        "Our DNA sequence is not transformed into k-mers (k=6).\n",
        "\n",
        "That is how we transform every DNA sequence sample in our dataset before we input it into the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fe2GnYbV-Wjh"
      },
      "outputs": [],
      "source": [
        "def tokenization(x): \n",
        "  return tokenizer(\" \".join(kmers(x[\"seq\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQJqYXcc_3Eb",
        "outputId": "cdf32326-05cc-46f8-ff89-e4ea2af2977b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'seq': 'ATGGAAAGAGGCACCATTCT'} \n",
            "\n",
            "- middle step that happens inside as we saw above:\n",
            "ATGGAA TGGAAA GGAAAG GAAAGA AAAGAG AAGAGG AGAGGC GAGGCA AGGCAC GGCACC GCACCA CACCAT ACCATT CCATTC CATTCT \n",
            "\n",
            "{'input_ids': [2, 501, 1989, 3848, 3089, 56, 212, 835, 3325, 999, 3983, 3629, 2214, 650, 2587, 2142, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "example = {'seq': 'ATGGAAAGAGGCACCATTCT'}\n",
        "print(example, '\\n')\n",
        "\n",
        "print('- middle step that happens inside as we saw above:')\n",
        "print(example_kmers, '\\n')\n",
        "\n",
        "example = tokenization(example)\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KaiXwao9_8SP",
        "outputId": "2bd3cf7c-41d0-4133-fff0-fe688a215007"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] ATGGAA TGGAAA GGAAAG GAAAGA AAAGAG AAGAGG AGAGGC GAGGCA AGGCAC GGCACC GCACCA CACCAT ACCATT CCATTC CATTCT [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tokenizer.decode(example['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWCNMSqrmyme"
      },
      "source": [
        "We saw how to tokenize one sample.\n",
        "\n",
        "An easy way to tokenize your whole dataset is to use the map() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCZhMrEvz1sK",
        "outputId": "efbcfc7b-cfaa-4b69-bd39-8cff8141258d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/simecek___parquet/simecek--human_nontata_promoters-5bc25970767ab8f4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-c16487ccf9e37cfb.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/simecek___parquet/simecek--human_nontata_promoters-5bc25970767ab8f4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-127a40767e83da86.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['labels', 'seq', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# map() takes a function and applies it to every sample in the dataset \n",
        "\n",
        "dataset_train_tokenized = dataset_train.map(tokenization, batched=False)\n",
        "dataset_test_tokenized = dataset_test.map(tokenization, batched=False)\n",
        "dataset_train_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqjwaf-FCAgH",
        "outputId": "8d130b56-02fe-4463-ef09-23de7975faec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 566, 2250, 795, 3165, 360, 1428, 1601, 2294, 972, 3874, 3195, 479, 1902, 3500, 1698, 2683, 2528, 1908, 3524, 1796, 3075, 4093, 4070, 3980, 3620, 2177, 503, 1999, 3887, 3246, 684, 2724, 2689, 2549, 1989, 3848, 3091, 62, 236, 931, 3712, 2548, 1985, 3831, 3021, 3879, 3215, 557, 2216, 660, 2625, 2296, 980, 3908, 3331, 1021, 4072, 3988, 3651, 2301, 997, 3976, 3601, 2104, 209, 824, 3282, 827, 3294, 875, 3485, 1638, 2443, 1566, 2155, 414, 1642, 2460, 1635, 2430, 1515, 1951, 3695, 2478, 1705, 2712, 2644, 2369, 1272, 979, 3902, 3305, 918, 3660, 2337, 1144, 466, 1850, 3292, 865, 3448, 1492, 1860, 3331, 1024, 4083, 4031, 3824, 2994, 3771, 2782, 2924, 3489, 1656, 2515, 1853, 3304, 916, 3649, 2296, 980, 3908, 3329, 1015, 4048, 3892, 3265, 759, 3024, 3892, 3268, 770, 3068, 4067, 3967, 3567, 1965, 3752, 2708, 2628, 2306, 1018, 4058, 3932, 3425, 1400, 1492, 1857, 3317, 965, 3848, 3089, 56, 212, 836, 3332, 1026, 4092, 4066, 3964, 3556, 1924, 3585, 2037, 4040, 3860, 3137, 247, 976, 3891, 3261, 742, 2956, 3619, 2174, 489, 1944, 3665, 2357, 1223, 782, 3114, 155, 605, 2408, 1425, 1592, 2259, 829, 3304, 914, 3642, 2267, 861, 3432, 1427, 1597, 2280, 916, 3650, 2300, 995, 3965, 3560, 1937, 3637, 2246, 780, 3108, 132, 513, 2040, 4050, 3898, 3289, 854, 3403, 1309, 1126, 396, 1572, 2180, 516, 2049, 4087, 4046, 3884, 3234, 636, 2532, 1924, 3585, 2040, 4049, 3893, 3272, 788, 3140, 260, 1027, 4096, 4084, 4034, 3836, 3044, 3972, 3]\n"
          ]
        }
      ],
      "source": [
        "# this is what later goes into our network as a one sample\n",
        "print(dataset_train_tokenized[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2RqMN40n1-2"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model pre-trained on human genome.\n",
        "\n",
        "You can find models like this on the Hugging Face web repository.\n"
      ],
      "metadata": {
        "id": "80EqTyU8jHiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"simecek/DNADebertaK6b\", num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN2Eby9QZXwm",
        "outputId": "9be790fe-c6f5-4aaa-9112-5ee54b9a54cf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at simecek/DNADebertaK6b were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at simecek/DNADebertaK6b and are newly initialized: ['classifier.weight', 'pooler.dense.bias', 'classifier.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "j5lKsYGRjGWD"
      },
      "outputs": [],
      "source": [
        "# # EXERCISE 1 --help\n",
        "# from transformers import DebertaConfig, DebertaForSequenceClassification\n",
        "\n",
        "# # First we need the model's configuration - the default settings are just fine for our goal\n",
        "# model_config = DebertaConfig(vocab_size=len(tokenizer.vocab), max_position_embeddings=512, num_hidden_layers=6, num_labels=2)\n",
        "# model = DebertaForSequenceClassification(config = model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r2s2kjsqB1Up"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-5\n",
        "EPOCHS = 3\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "        output_dir='outputs', \n",
        "        learning_rate=LEARNING_RATE, \n",
        "        num_train_epochs=EPOCHS, \n",
        "        evaluation_strategy=\"epoch\", \n",
        "        logging_strategy='epoch',\n",
        "        per_device_train_batch_size=BATCH_SIZE, \n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        fp16=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QOR8Bkqvfe4q"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnEKMRRpRJOR",
        "outputId": "2fe74c0b-0c48-40f6-c44e-c4dcd633daca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.trainer.Trainer at 0x7f8712525510>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=dataset_train_tokenized,\n",
        "    eval_dataset=dataset_test_tokenized,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "4EevzTIKoCNR",
        "outputId": "83c8632f-a510-4078-87ea-2bee604f22b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 96\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 00:51, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.668900</td>\n",
              "      <td>0.621052</td>\n",
              "      <td>0.781500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.577900</td>\n",
              "      <td>0.557015</td>\n",
              "      <td>0.788000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.532800</td>\n",
              "      <td>0.539262</td>\n",
              "      <td>0.795500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 32\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 32\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 32\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=96, training_loss=0.5932058691978455, metrics={'train_runtime': 53.2371, 'train_samples_per_second': 56.352, 'train_steps_per_second': 1.803, 'total_flos': 192471118560000.0, 'train_loss': 0.5932058691978455, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "1.   Exchange our current model for a **NOT** pre-trained model and compare \n",
        " - you will find a hint in the code above\n",
        "2.   Our DNA-pre-trained model is not the only one you can find on the Hugging Face repository. Try to exchange our pre-trained model for a different pre-trained model\n",
        " - for example this one [https://huggingface.co/armheb/DNA_bert_6](https://huggingface.co/armheb/DNA_bert_6)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M2Sffh-eWzYr"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "environment": {
      "name": "pytorch-gpu.1-9.m75",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "af167c304fdc99884e31a029277e630a5b00036f91292350fffdeed1d15b16ff"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit ('genomic_benchmarks': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}