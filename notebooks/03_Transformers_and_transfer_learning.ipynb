{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML-Bioinfo-CEITEC/ECCB2022/blob/main/notebooks/03_Transformers_and_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7DUHcKNLkJp"
      },
      "source": [
        "# Basic transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF_kvRbryibQ"
      },
      "source": [
        "## Setup & Data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIcaZLWXy12J"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "4vZiFzjt7ZtV"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets tokenizers --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lrwpPk9y_AX"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB3x6DnHaDuP",
        "outputId": "5626ee4e-5958-43fa-f240-64f50e461a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration simecek--human_nontata_promoters-5bc25970767ab8f4\n",
            "WARNING:datasets.builder:Reusing dataset parquet (/root/.cache/huggingface/datasets/simecek___parquet/simecek--human_nontata_promoters-5bc25970767ab8f4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['labels', 'seq'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "DATASET_NAME = \"simecek/human_nontata_promoters\"\n",
        "\n",
        "# take a small portion of the dataset for time purposes\n",
        "# the fist and the last 1000 samples because this specific dataset is ordered (positive, negative samples)\n",
        "dataset_train = load_dataset(DATASET_NAME, split='train[:500]+train[-500:]')\n",
        "dataset_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9aq8AMIU7E-",
        "outputId": "3594a189-c65b-46fc-c3fb-7ef8ce4853e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration simecek--human_nontata_promoters-5bc25970767ab8f4\n",
            "WARNING:datasets.builder:Reusing dataset parquet (/root/.cache/huggingface/datasets/simecek___parquet/simecek--human_nontata_promoters-5bc25970767ab8f4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['labels', 'seq'],\n",
              "    num_rows: 2000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "dataset_test = load_dataset(DATASET_NAME, split='test[:1000]+test[-1000:]')\n",
        "dataset_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fa9tyYQqkrH",
        "outputId": "cc2faed8-ca7c-4ce7-9c9e-fab752e59d09"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 0,\n",
              " 'seq': 'ACAGATTCAGGATGTCCTGTCGGGGCATGGACCCTGGAAAGCTGCGGACACCAGGAGGGCAGGCAAGAGAGTCTCATCTCTTGCTCCCTAGGAGCTATGAGTTGAGGGCGCCGTCTGAGCAGGAGGGACGGACGGGTGCCCAGGGTTTGAGGAAAGAGGGGTGTGGGAAGGACGCATGCTAGAACTTCAGAGCAGTTCAGCAGGTGCAGAATGGGAGTTATCATGGGGACTGTGGGAGAAGGGGCGGTGGG'}"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "# one training sample sequence and its label\n",
        "dataset_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXMgg43Fypoy"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLnNCZbi69ba"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzhurWwCgO2s"
      },
      "source": [
        "You can find useful models and their respective tokenizers right on the Hugging Face repository.\n",
        "\n",
        "You can use the premade tokenizers even for your own model if they fit your task/purpose.\n",
        "\n",
        "If you don't find existing tokenizers for your usecase, the Hugging Face documentation contains simple tutorials to create your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9ZGgWM_YF28",
        "outputId": "ec49becb-5295-4639-b646-3c38f6f21d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--armheb--DNA_bert_6/snapshots/a79a8fd96ad172f964a4dbef3f4d7545a5034baa/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"armheb/DNA_bert_6\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_rnn_layer\": 1,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"rnn\": \"lstm\",\n",
            "  \"rnn_dropout\": 0.0,\n",
            "  \"rnn_hidden\": 768,\n",
            "  \"split\": 10,\n",
            "  \"transformers_version\": \"4.22.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 4101\n",
            "}\n",
            "\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--armheb--DNA_bert_6/snapshots/a79a8fd96ad172f964a4dbef3f4d7545a5034baa/vocab.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--armheb--DNA_bert_6/snapshots/a79a8fd96ad172f964a4dbef3f4d7545a5034baa/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--armheb--DNA_bert_6/snapshots/a79a8fd96ad172f964a4dbef3f4d7545a5034baa/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--armheb--DNA_bert_6/snapshots/a79a8fd96ad172f964a4dbef3f4d7545a5034baa/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"armheb/DNA_bert_6\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_rnn_layer\": 1,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"rnn\": \"lstm\",\n",
            "  \"rnn_dropout\": 0.0,\n",
            "  \"rnn_hidden\": 768,\n",
            "  \"split\": 10,\n",
            "  \"transformers_version\": \"4.22.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 4101\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--armheb--DNA_bert_6/snapshots/a79a8fd96ad172f964a4dbef3f4d7545a5034baa/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"armheb/DNA_bert_6\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_ids\": 0,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_rnn_layer\": 1,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"rnn\": \"lstm\",\n",
            "  \"rnn_dropout\": 0.0,\n",
            "  \"rnn_hidden\": 768,\n",
            "  \"split\": 10,\n",
            "  \"transformers_version\": \"4.22.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 4101\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"armheb/DNA_bert_6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt-5htSY-5OT"
      },
      "source": [
        "The tokenizers expects sequences of tokens separated by spaces. \n",
        "\n",
        "Therefore, we will move a sliding window along the sequence and extract k-mers of k=6 (6 characters long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "_9kMLG0Smcck"
      },
      "outputs": [],
      "source": [
        "def kmers(s, k=6):\n",
        "  return [s[i:i + k] for i in range(0, len(s)-k+1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZNz-o7C_Tcc"
      },
      "source": [
        "Our k-mers function turns a sequence into a list of k-mers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJQrL17nspOJ",
        "outputId": "dedda7cd-c0bf-40d1-a499-27ecc89d5c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ATGGAAAGAGGCACCATTCT\n",
            "['ATGGAA', 'TGGAAA', 'GGAAAG', 'GAAAGA', 'AAAGAG', 'AAGAGG', 'AGAGGC', 'GAGGCA', 'AGGCAC', 'GGCACC', 'GCACCA', 'CACCAT', 'ACCATT', 'CCATTC', 'CATTCT']\n"
          ]
        }
      ],
      "source": [
        "example = 'ATGGAAAGAGGCACCATTCT'\n",
        "print(example)\n",
        "\n",
        "example = kmers(example)\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE4Jy-I5jn8b"
      },
      "source": [
        "We are not done yet. \n",
        "\n",
        "Now we have to concatinate the k-mers separated by spaces into a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yldFD_v7j4nd",
        "outputId": "0c673414-510b-4918-cf68-5acc4802b5f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ATGGAAAGAGGCACCATTCT\n",
            "ATGGAA TGGAAA GGAAAG GAAAGA AAAGAG AAGAGG AGAGGC GAGGCA AGGCAC GGCACC GCACCA CACCAT ACCATT CCATTC CATTCT\n"
          ]
        }
      ],
      "source": [
        "example = 'ATGGAAAGAGGCACCATTCT'\n",
        "print(example)\n",
        "\n",
        "example_kmers = \" \".join(kmers(example))\n",
        "print(example_kmers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9BSy2M7kv2U"
      },
      "source": [
        "Our DNA sequence is not transformed into k-mers (k=6).\n",
        "\n",
        "That is how we transform every DNA sequence sample in our dataset before we input it into the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "Fe2GnYbV-Wjh"
      },
      "outputs": [],
      "source": [
        "def tokenization(x): \n",
        "  return tokenizer(\" \".join(kmers(x[\"seq\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQJqYXcc_3Eb",
        "outputId": "d332d397-1987-49b4-df7e-60f915c9a31b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'seq': 'ATGGAAAGAGGCACCATTCT'} \n",
            "\n",
            "- middle step that happens inside as we saw above:\n",
            "ATGGAA TGGAAA GGAAAG GAAAGA AAAGAG AAGAGG AGAGGC GAGGCA AGGCAC GGCACC GCACCA CACCAT ACCATT CCATTC CATTCT \n",
            "\n",
            "{'input_ids': [2, 501, 1989, 3848, 3089, 56, 212, 835, 3325, 999, 3983, 3629, 2214, 650, 2587, 2142, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ],
      "source": [
        "example = {'seq': 'ATGGAAAGAGGCACCATTCT'}\n",
        "print(example, '\\n')\n",
        "\n",
        "print('- middle step that happens inside as we saw above:')\n",
        "print(example_kmers, '\\n')\n",
        "\n",
        "example = tokenization(example)\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KaiXwao9_8SP",
        "outputId": "fb47b055-31e9-4769-cc06-746159147421"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] ATGGAA TGGAAA GGAAAG GAAAGA AAAGAG AAGAGG AGAGGC GAGGCA AGGCAC GGCACC GCACCA CACCAT ACCATT CCATTC CATTCT [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "tokenizer.decode(example['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWCNMSqrmyme"
      },
      "source": [
        "We saw how to tokenize one sample.\n",
        "\n",
        "An easy way to tokenize your whole dataset is to use the map() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCZhMrEvz1sK",
        "outputId": "625add62-1369-4052-af5c-6934cc04b990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/simecek___parquet/simecek--human_nontata_promoters-5bc25970767ab8f4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-c16487ccf9e37cfb.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/simecek___parquet/simecek--human_nontata_promoters-5bc25970767ab8f4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-127a40767e83da86.arrow\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['labels', 'seq', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "    num_rows: 1000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "# map() takes a function and applies it to every sample in the dataset \n",
        "\n",
        "dataset_train_tokenized = dataset_train.map(tokenization, batched=False)\n",
        "dataset_test_tokenized = dataset_test.map(tokenization, batched=False)\n",
        "dataset_train_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqjwaf-FCAgH",
        "outputId": "a0fb6243-0a6f-42fb-f450-e7170e75fd6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 566, 2250, 795, 3165, 360, 1428, 1601, 2294, 972, 3874, 3195, 479, 1902, 3500, 1698, 2683, 2528, 1908, 3524, 1796, 3075, 4093, 4070, 3980, 3620, 2177, 503, 1999, 3887, 3246, 684, 2724, 2689, 2549, 1989, 3848, 3091, 62, 236, 931, 3712, 2548, 1985, 3831, 3021, 3879, 3215, 557, 2216, 660, 2625, 2296, 980, 3908, 3331, 1021, 4072, 3988, 3651, 2301, 997, 3976, 3601, 2104, 209, 824, 3282, 827, 3294, 875, 3485, 1638, 2443, 1566, 2155, 414, 1642, 2460, 1635, 2430, 1515, 1951, 3695, 2478, 1705, 2712, 2644, 2369, 1272, 979, 3902, 3305, 918, 3660, 2337, 1144, 466, 1850, 3292, 865, 3448, 1492, 1860, 3331, 1024, 4083, 4031, 3824, 2994, 3771, 2782, 2924, 3489, 1656, 2515, 1853, 3304, 916, 3649, 2296, 980, 3908, 3329, 1015, 4048, 3892, 3265, 759, 3024, 3892, 3268, 770, 3068, 4067, 3967, 3567, 1965, 3752, 2708, 2628, 2306, 1018, 4058, 3932, 3425, 1400, 1492, 1857, 3317, 965, 3848, 3089, 56, 212, 836, 3332, 1026, 4092, 4066, 3964, 3556, 1924, 3585, 2037, 4040, 3860, 3137, 247, 976, 3891, 3261, 742, 2956, 3619, 2174, 489, 1944, 3665, 2357, 1223, 782, 3114, 155, 605, 2408, 1425, 1592, 2259, 829, 3304, 914, 3642, 2267, 861, 3432, 1427, 1597, 2280, 916, 3650, 2300, 995, 3965, 3560, 1937, 3637, 2246, 780, 3108, 132, 513, 2040, 4050, 3898, 3289, 854, 3403, 1309, 1126, 396, 1572, 2180, 516, 2049, 4087, 4046, 3884, 3234, 636, 2532, 1924, 3585, 2040, 4049, 3893, 3272, 788, 3140, 260, 1027, 4096, 4084, 4034, 3836, 3044, 3972, 3]\n"
          ]
        }
      ],
      "source": [
        "# this is what later goes into our network as a one sample\n",
        "print(dataset_train_tokenized[0]['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2RqMN40n1-2"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model pre-trained on human genome.\n",
        "\n",
        "You can find models like this on the Hugging Face web repository.\n"
      ],
      "metadata": {
        "id": "80EqTyU8jHiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"simecek/DNADebertaK6b\", num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN2Eby9QZXwm",
        "outputId": "1952b000-84a1-4708-984b-e292deef4aeb"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--simecek--DNADebertaK6b/snapshots/d045450497234887e1e211066effec712575374e/config.json\n",
            "Model config DebertaConfig {\n",
            "  \"_name_or_path\": \"simecek/DNADebertaK6b\",\n",
            "  \"architectures\": [\n",
            "    \"DebertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-07,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"max_relative_positions\": -1,\n",
            "  \"model_type\": \"deberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_dropout\": 0,\n",
            "  \"pooler_hidden_act\": \"gelu\",\n",
            "  \"pooler_hidden_size\": 768,\n",
            "  \"pos_att_type\": null,\n",
            "  \"position_biased_input\": true,\n",
            "  \"relative_attention\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.22.1\",\n",
            "  \"type_vocab_size\": 0,\n",
            "  \"vocab_size\": 4101\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--simecek--DNADebertaK6b/snapshots/d045450497234887e1e211066effec712575374e/pytorch_model.bin\n",
            "Some weights of the model checkpoint at simecek/DNADebertaK6b were not used when initializing DebertaForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at simecek/DNADebertaK6b and are newly initialized: ['classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "j5lKsYGRjGWD"
      },
      "outputs": [],
      "source": [
        "# # EXERCISE 1 --help\n",
        "# from transformers import DebertaConfig, DebertaForSequenceClassification\n",
        "\n",
        "# # First we need the model's configuration - the default settings are just fine for our goal\n",
        "# model_config = DebertaConfig(vocab_size=len(tokenizer.vocab), max_position_embeddings=512, num_hidden_layers=6, num_labels=2)\n",
        "# model = DebertaForSequenceClassification(config = model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "r2s2kjsqB1Up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfde4904-3983-4446-c730-2ebff7d226b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-5\n",
        "EPOCHS = 3\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "        'outputs', \n",
        "        learning_rate=LEARNING_RATE, \n",
        "        fp16=True,\n",
        "        evaluation_strategy=\"epoch\", \n",
        "        per_device_train_batch_size=BATCH_SIZE, \n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        num_train_epochs=EPOCHS, \n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "QOR8Bkqvfe4q"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnEKMRRpRJOR",
        "outputId": "b2cae289-a411-4f02-e17d-d4e7dcce85e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cuda_amp half precision backend\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.trainer.Trainer at 0x7f3e600bde50>"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=dataset_train_tokenized,\n",
        "    eval_dataset=dataset_test_tokenized,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "id": "4EevzTIKoCNR",
        "outputId": "75e3367c-29b5-47a6-c31a-cea5b0498f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the training set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1000\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 96\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [96/96 01:09, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.614922</td>\n",
              "      <td>0.779500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.546625</td>\n",
              "      <td>0.791500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.531640</td>\n",
              "      <td>0.794500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 32\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 32\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/deberta/modeling_deberta.py:679: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  query_layer = query_layer / torch.tensor(scale, dtype=query_layer.dtype)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `DebertaForSequenceClassification.forward` and have been ignored: seq. If seq are not expected by `DebertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 2000\n",
            "  Batch size = 32\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=96, training_loss=0.5887002944946289, metrics={'train_runtime': 70.094, 'train_samples_per_second': 42.8, 'train_steps_per_second': 1.37, 'total_flos': 192471118560000.0, 'train_loss': 0.5887002944946289, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "1.   Exchange our current model for a **NOT** pre-trained model and compare \n",
        " - you will find a hint in the code above\n",
        "2.   Our DNA-pre-trained model is not the only one you can find on the Hugging Face repository. Try to exchange our pre-trained model for a different pre-trained model\n",
        " - for example this one [https://huggingface.co/armheb/DNA_bert_6](https://huggingface.co/armheb/DNA_bert_6)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M2Sffh-eWzYr"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "environment": {
      "name": "pytorch-gpu.1-9.m75",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m75"
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "af167c304fdc99884e31a029277e630a5b00036f91292350fffdeed1d15b16ff"
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit ('genomic_benchmarks': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}